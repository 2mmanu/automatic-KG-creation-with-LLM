The model weights are updated by retraining with new data every time a new labeled dataset is created. This is typically done after the analysts have manually reviewed and audited the output of the existing DL models trained to classify events of interest. The new labeled dataset is then used to retrain the model, updating the model weights with the new information. The frequency of this process depends on the availability of new data and the resources available for manual review and auditing. Keywords: model weights, retraining, new data, labeled dataset, manual review, auditing, frequency. Abstract Conservation Metrics, Inc. has developed a deep learning approach to large scale biodiversity monitoring. The approach has allowed one client to grow the scale of a key project by two orders of magnitude from 600 hours of monitoring in the summer of 2012 to almost 80,000 hours of monitoring in the summer of 2014. This effort has transformed their understanding of wildlife impacts, and doubled the funding for mitigation and monitoring efforts in the area. The deep learning approach has also allowed for the creation of labeled datasets to train and refine deep learning models, and for the manual review and auditing of the output of existing deep learning models trained to classify events of interest. Keywords: biodiversity monitoring, deep learning, large scale, conservation, wildlife, data exploration, labeled datasets, manual review, auditing, model weights, retraining. Deep Learning for Large Scale Biodiversity Monitoring Conservation Metrics, Inc. Data exploration is required at the initial phases of the analysis process. Analysts sort and filter the data according to date ranges, time of day, or site location. Additionally and importantly, they also apply conditionals regarding the elemental attributes of the signals. For example, for audio signals, analysts can sort the data frequency ranges, click-like or whistle-like sounds, rising tones or falling tones, repetitive pulses, and so on. Visually, one can specify fast or slow moving objects, large or small objects, the presence of eye-shine, and certain colors. Modalities are combined as well. For example, images can be selected according to time periods when a specified sound occurs. These exploration tools have proven critical for rapidly building datasets to train deep learning models for new species, and for finding novelty in the data for which no models yet exist. A roadmap of improvements is envisioned for further enhancing analysts' ability to explore and search large volumes of sensor data, including providing a richer set of elemental signal attributes, a more natural-language interface, and also the ability to search by exemplars. Other exploration tools are more focused on visualizing the data. For example, the software can generate heat maps of animal activity, or display the data in a spectrogram format to show the frequency content of audio signals over time. These tools can help analysts quickly identify patterns and trends in the data, and can also be used to evaluate the performance of deep learning models. Finally, the software expedites auditing â€“ the manual review of classification model output. We run existing deep learning models against our survey data to automatically classify and detect species or events of interest for each project. These models output the relative probability that an event is from a specific data class (usually corresponding to a species or event), and analysts can sort the data accordingly. Analysts are then presented with ranked lists of events, and can manually review and label the data to create new labeled datasets. These datasets can then be used to retrain the deep learning models, updating the model weights with the new information. In summary, the deep learning approach developed by Conservation Metrics, Inc. has revolutionized large scale biodiversity monitoring. The approach has allowed for the creation of labeled datasets to train and refine deep learning models, and for the manual review and auditing of the output of existing deep learning models trained to classify events of interest. The approach has also provided analysts with powerful tools for data exploration and visualization, and has expedited the auditing process. These advances have allowed for the scaling of projects by an order of magnitude while maintaining costs, and have transformed the understanding of wildlife impacts and the funding for mitigation and monitoring efforts. 