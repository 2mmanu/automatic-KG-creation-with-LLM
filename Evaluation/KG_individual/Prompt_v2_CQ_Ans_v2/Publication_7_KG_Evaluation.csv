Publication No.,CQ No.,CQ,CQ LLM Answer,Ontology Concept,KG Individual,KG Individual Label
7,1,What data formats are used in the deep learning pipeline?,"The provided context does not explicitly mention the data formats used in the deep learning pipeline. However, it is mentioned that square patches were extracted with the labelled pixel at their centre and there is no restriction on the size of these patches. It can be inferred that the data format used for these patches is likely to be image files such as JPEG or PNG. Additionally, the classiﬁcation process uses a 2-layered neural network, which typically takes ﬂat arrays of numerical data as input. Therefore, it can be inferred that the image patches are likely to be converted into ﬂat arrays of numerical data before being fed into the neural network. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_1,Image files such as JPEG or PNG
7,2,What are the sources of input data for the deep learning pipeline?,The input data for the deep learning pipeline comes from unlabelled coral mosaics of three sites of the Abrolhos Islands coral reef from two years. ,InputData,https://w3id.org/dlprovenance#InputData_1,Unlabelled coral mosaics of three sites of the Abrolhos Islands coral reef from two years
7,3,How was raw data collected in terms of methods and tools?,"The raw data was collected using an Autonomous Underwater Vehicle (AUV) equipped with a high- resolution camera. The AUV was used to capture images of the coral reefs in the Abrolhos Islands. The images were then used to generate mosaics and coral maps, which were used to quantify the coral population and analyze the trends in coral population. ",Method,https://w3id.org/dlprovenance#Method_1,Autonomous Underwater Vehicle (AUV) equipped with a high-resolution camera
7,3,How was raw data collected in terms of methods and tools?,"The raw data was collected using an Autonomous Underwater Vehicle (AUV) equipped with a high- resolution camera. The AUV was used to capture images of the coral reefs in the Abrolhos Islands. The images were then used to generate mosaics and coral maps, which were used to quantify the coral population and analyze the trends in coral population. ",Method,https://w3id.org/dlprovenance#Method_2,Not specified
7,3,How was raw data collected in terms of methods and tools?,"The raw data was collected using an Autonomous Underwater Vehicle (AUV) equipped with a high- resolution camera. The AUV was used to capture images of the coral reefs in the Abrolhos Islands. The images were then used to generate mosaics and coral maps, which were used to quantify the coral population and analyze the trends in coral population. ",Tool,https://w3id.org/dlprovenance#Tool_1,Image patches are likely to be converted into ﬂat arrays of numerical data before being fed into the neural network
7,4,"Is the source code openly accessible, and if so, what is the repository link?",The source code is not openly accessible in the provided context. ,RepositoryLink,https://w3id.org/dlprovenance#RepositoryLink_1,Not openly accessible
7,5,What preprocessing steps are involved before training the deep learning model?,"The provided context does not explicitly mention the preprocessing steps taken before training the deep learning model. However, it does mention that square patches were extracted with the labelled pixel at their centre, and there is no restriction on the size of these patches. Instead of using the whole image for training, patches at multiple scales centred around the given labelled pixels were extracted. This technique is termed as spatial pyramid pooling (SPP). Therefore, it can be inferred that some form of patch extraction and resizing is involved in the preprocessing step. ",PreprocessingStep,https://w3id.org/dlprovenance#PreprocessingStep_1,"Square patches were extracted with the labelled pixel at their centre, and there is no restriction on the size of these patches. Instead of using the whole image for training, patches at multiple scales centred around the given labelled pixels were extracted. This technique is termed as spatial pyramid pooling (SPP)."
7,6,Are there transformations or augmentations applied to the input data?,"Yes, the authors applied color channel stretch on each image in the dataset. They calculated the 1% and 99% intensity percentiles for each color channel, subtracted the lower intensity from all the intensities in each respective channel, and divided these intensities by the upper percentile. This resulted in better performance compared to the original intensities. ",TransformationAugmentation,https://w3id.org/dlprovenance#TransformationAugmentation_1,Color channel stretch on each image in the dataset
7,7,Does the paper discuss data bias or ethical implications?,"No, the paper does not discuss data bias or ethical implications. ",Bias,https://w3id.org/dlprovenance#Bias_1,Not discussed
7,7,Does the paper discuss data bias or ethical implications?,"No, the paper does not discuss data bias or ethical implications. ",EthicalImplication,https://w3id.org/dlprovenance#EthicalImplication_1,Not specified
7,8,What is the architecture of the deep learning model in the pipeline?,The architecture of the deep learning model in the pipeline is a 2-layered neural network used to classify corals from non-corals. The model uses features learned using a Convolutional Neural Network (CNN) and the Spatial Pyramid Pooling (SPP) technique for patch extraction. The SPP technique makes the resulting features scale invariant. ,Architecture,https://w3id.org/dlprovenance#Architecture_1,2-layered neural network used to classify corals from non-corals
7,8,What is the architecture of the deep learning model in the pipeline?,The architecture of the deep learning model in the pipeline is a 2-layered neural network used to classify corals from non-corals. The model uses features learned using a Convolutional Neural Network (CNN) and the Spatial Pyramid Pooling (SPP) technique for patch extraction. The SPP technique makes the resulting features scale invariant. ,Model,https://w3id.org/dlprovenance#Model_1,Not specified
7,9,How was the model selected for a specific task?,The model was selected based on the performance of the classifier on the Benthoz15 dataset. The authors achieved a classification accuracy greater than 90% in all of their experiments. The model was then applied to the coral reef of the Abrolhos Islands to investigate the trends in coral population and generate coral maps for this region. The framework detected the decreasing trend in the coral population of this region as well. ,Model,https://w3id.org/dlprovenance#Model_1,Not specified
7,10,What were the considerations in the model selection process?,The model selection process considered the use of pre-trained CNN image representations extracted from VGGnet to a coral reef classiﬁcation problem. The authors investigated the effectiveness of their trained clas- siﬁer on unlabelled coral mosaics of the Abrolhos Islands. They generated coral maps for this region and quantiﬁed the coral population automatically. The framework detected the decreasing trend in the coral population of this region as well. The proposed framework is an important step towards investigating the long-term effects of environmental change on the effective sustenance of marine ecosystems automatically. ,Consideration,https://w3id.org/dlprovenance#Consideration_1,Not specified
7,11,How many models are used in the pipeline?,The text does not provide information on the number of models used in the pipeline. ,NumberOfModels,https://w3id.org/dlprovenance#NumberOfModels_1,Not specified
7,12,Are the models considered state-of-the-art?,The text does not provide enough information to determine if the models are state-of-the-art or not. ,StateOfTheArt,https://w3id.org/dlprovenance#StateOfTheArt_1,Not specified
7,12,Are the models considered state-of-the-art?,The text does not provide enough information to determine if the models are state-of-the-art or not. ,Model,https://w3id.org/dlprovenance#Model_1,Not specified
7,13,How is the model initialized?,The model is initialized using pre-trained CNN image representations extracted from VGGnet. These representations are then fine-tuned for the coral reef classiﬁcation problem. ,Model,https://w3id.org/dlprovenance#Model_1,Not specified
7,14,Are there specific weight configurations used during initialization?,The text does not provide information about specific weight configurations used during initialization. ,WeightConfiguration,https://w3id.org/dlprovenance#WeightConfiguration_1,Not specified
7,15,Are there optimization algorithms or learning rate schedules used during training?,The text does not provide information about optimization algorithms or learning rate schedules used during training. ,OptimizationAlgorithm,https://w3id.org/dlprovenance#OptimizationAlgorithm_1,Not specified
7,15,Are there optimization algorithms or learning rate schedules used during training?,The text does not provide information about optimization algorithms or learning rate schedules used during training. ,LearningRateSchedule,https://w3id.org/dlprovenance#LearningRateSchedule_1,Not specified
7,16,What is the convergence criteria or stopping condition for the training process?,The provided context does not contain information about the convergence criteria or stopping condition for the training process. ,ConvergenceCriteria,https://w3id.org/dlprovenance#ConvergenceCriteria_1,Not specified
7,17,Which software frameworks or libraries are used to build the model?,The paper does not provide information on the specific software frameworks or libraries used to build the model. ,SoftwareFrameworkLibrary,https://w3id.org/dlprovenance#SoftwareFrameworkLibrary_1,Not specified
7,18,What hardware infrastructures are used for model training?,The text does not provide information about the hardware infrastructures used for model training. ,HardwareInfrastructure,https://w3id.org/dlprovenance#HardwareInfrastructure_1,Not specified
7,19,What hyperparameters are used in the model?,The text does not provide information about the hyperparameters used in the model. ,Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_1,Not specified
7,19,What hyperparameters are used in the model?,The text does not provide information about the hyperparameters used in the model. ,Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_2,Number of neurons in the hidden layers were optimized for maximum performance
7,20,Why were those specific hyperparameters selected?,The text does not provide information on why those specific hyperparameters were selected. It only mentions that the number of neurons in the hidden layers were optimized for maximum performance. ,Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_1,Not specified
7,20,Why were those specific hyperparameters selected?,The text does not provide information on why those specific hyperparameters were selected. It only mentions that the number of neurons in the hidden layers were optimized for maximum performance. ,Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_2,Number of neurons in the hidden layers were optimized for maximum performance
7,21,Are the provided hyperparameters fine-tuned?,"The provided text does not explicitly mention whether the hyperparameters are fine-tuned or not. However, it does mention that the weights of the deep network are ﬁne tuned using the Benthoz15 dataset. It is possible that the hyperparameters are also ﬁne tuned during this process, but the text does not provide enough information to confirm this. ",FineTuning,https://w3id.org/dlprovenance#FineTuning_1,Not specified
7,24,Is there sufficient information to reproduce the deep learning pipeline?,"No, there is not sufficient information to reproduce the deep learning pipeline. The paper provides a high-level overview of the proposed framework, but it lacks the necessary details to replicate the pipeline. For instance, it does not specify the architecture of the 2-layered neural network used for classifying corals from non-corals, nor does it provide the code or data used in the study. ",Reproducibility,https://w3id.org/dlprovenance#Reproducibility_1,No
7,25,What measures are taken to explain model predictions?,The text does not provide information on measures taken to explain model predictions. ,Explanation,https://w3id.org/dlprovenance#Explanation_1,Not specified
7,25,What measures are taken to explain model predictions?,The text does not provide information on measures taken to explain model predictions. ,PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Corals from non-corals
7,26,What is the versioning strategy for trained models?,The text does not provide information about the versioning strategy for trained models. ,VersioningStrategy,https://w3id.org/dlprovenance#VersioningStrategy_1,Not specified
7,27,How are different versions of datasets managed?,The provided context does not contain information about how different versions of datasets are managed. ,DatasetVersion,https://w3id.org/dlprovenance#DatasetVersion_1,Not specified
7,28,How are updates to datasets documented?,The provided context does not contain information on how updates to datasets are documented. ,UpdateFrequency,https://w3id.org/dlprovenance#UpdateFrequency_1,Not specified
7,28,How are updates to datasets documented?,The provided context does not contain information on how updates to datasets are documented. ,UpdateFrequency,https://w3id.org/dlprovenance#UpdateFrequency_2,Not specified
7,29,What annotations or labels are associated with the data?,The data is annotated with pixel labels. Each training image has 50 pixels marked with corresponding ground truth labels. The labels are coral or non-coral. ,PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Corals from non-corals
7,31,What predictions or classifications are generated by the deep learning model?,The deep learning model generates classifications of coral reefs into coral or non-coral categories. ,PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Corals from non-corals
7,32,How is uncertainty or confidence in model predictions captured?,"The paper does not explicitly mention how uncertainty or confidence in model predictions is captured. However, it does mention that the proposed method automatically quantifies coral coverage in the Abrolhos Islands, Western Australia. This suggests that the method provides some measure of certainty or uncertainty in its predictions, but the specifics are not provided in the text. ",UncertaintyConfidence,https://w3id.org/dlprovenance#UncertaintyConfidence_1,Not specified
7,32,How is uncertainty or confidence in model predictions captured?,"The paper does not explicitly mention how uncertainty or confidence in model predictions is captured. However, it does mention that the proposed method automatically quantifies coral coverage in the Abrolhos Islands, Western Australia. This suggests that the method provides some measure of certainty or uncertainty in its predictions, but the specifics are not provided in the text. ",UncertaintyConfidence,https://w3id.org/dlprovenance#UncertaintyConfidence_2,Not specified
7,33,Are there post-processing steps applied to the model's output?,"Yes, there are post-processing steps applied to the model's output. The classiﬁer's output is post-processed to generate coral maps for the Abrolhos Islands. The coral maps are generated by applying a threshold on the classiﬁer's output. The threshold is determined based on the classiﬁer's performance on the Benthoz15 dataset. The coral maps are then used to quantify the coral population automatically. ",PostProcessing,https://w3id.org/dlprovenance#PostProcessing_1,Threshold is determined based on the classifier’s performance on the Benthoz15 dataset
7,34,"Is the trained model deployed, and if not, what is the reason?","The provided context does not mention whether the trained model is deployed or not. It only discusses the training and testing of the model on different datasets. Therefore, it is not possible to answer this question without additional information. ",Deployment,https://w3id.org/dlprovenance#Deployment_1,Not specified
7,35,What hardware and software are used for model deployment?,The text does not provide information on the hardware and software used for model deployment. ,SoftwareFrameworkLibrary,https://w3id.org/dlprovenance#SoftwareFrameworkLibrary_1,Not specified
7,35,What hardware and software are used for model deployment?,The text does not provide information on the hardware and software used for model deployment. ,HardwareInfrastructure,https://w3id.org/dlprovenance#HardwareInfrastructure_1,Not specified
7,37,What ethical considerations are taken into account during development and deployment?,The text does not provide information about the ethical considerations taken into account during development and deployment. ,EthicalImplication,https://w3id.org/dlprovenance#EthicalImplication_1,Not specified
7,38,How is bias in the data addressed?,The text does not provide information on how bias in the data is addressed. ,BiasAddressing,https://w3id.org/dlprovenance#BiasAddressing_1,Not specified
7,39,Is there transparency in the decision-making process regarding bias?,The text does not provide information about transparency in the decision-making process regarding bias. ,Transparency,https://w3id.org/dlprovenance#Transparency_1,Not specified
7,40,Are privacy and security measures implemented in handling sensitive data?,The text does not provide information about privacy and security measures implemented in handling sensitive data. ,PrivacySecurityMeasure,https://w3id.org/dlprovenance#PrivacySecurityMeasure_1,Not specified
7,40,Are privacy and security measures implemented in handling sensitive data?,The text does not provide information about privacy and security measures implemented in handling sensitive data. ,SensitiveData,https://w3id.org/dlprovenance#SensitiveData_1,Not specified
