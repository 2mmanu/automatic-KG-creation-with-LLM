Publication No.,CQ No.,CQ,CQ LLM Answer,Ontology Concept,KG Individual,KG Individual Label
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_1,Raw data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_2,Preprocessed data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_3,Training data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_4,Validation data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_5,Test data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_6,Model data
3,1,What data formats are used in the deep learning pipeline?,"The deep learning pipeline typically involves the use of various data formats, including: 1. Raw data: This is the initial data format, which can be in various forms such as images, audio files, text files, etc. 2. Preprocessed data: This is the data format obtained after preprocessing the raw data. Preprocessing can involve various steps such as data cleaning, data augmentation, normalization, etc. 3. Training data: This is the data format used to train the deep learning model. It is typically a subset of the preprocessed data, and is used to train the model by adjusting the model's parameters to minimize the error between the predicted and actual values. 4. Validation data: This is the data format used to validate the deep learning model during the training process. It is a subset of the preprocessed data that is separate from the training data, and is used to evaluate the model's performance on unseen data. 5. Test data: This is the data format used to evaluate the final performance of the deep learning model. It is a subset of the preprocessed data that is separate from the training and validation data, and is used to assess the model's ability to generalize to new, unseen data. 6. Model data: This is the data format used to store the trained deep learning model. It typically includes the model's architecture, weights, and other parameters that have been learned during the training process. 7. Prediction data: This is the data format used to make predictions using the trained deep learning model. It can be in various forms such as images, audio files, text files, etc. , and is used as input to the model to generate predictions. ",DataFormat,https://w3id.org/dlprovenance#DataFormat_7,Prediction data
3,2,What are the sources of input data for the deep learning pipeline?,"The input data for the deep learning pipeline comes from the Macaulay Library, which is a library of natural sounds. The data includes 20 recognized species of Platymantis, with 20 recordings per species used to generate 400 spectrograms for the identification challenge. ",InputData,https://w3id.org/dlprovenance#InputData_1,Macaulay Library
3,2,What are the sources of input data for the deep learning pipeline?,"The input data for the deep learning pipeline comes from the Macaulay Library, which is a library of natural sounds. The data includes 20 recognized species of Platymantis, with 20 recordings per species used to generate 400 spectrograms for the identification challenge. ",InputData,https://w3id.org/dlprovenance#InputData_2,20 recognized species of Platymantis
3,2,What are the sources of input data for the deep learning pipeline?,"The input data for the deep learning pipeline comes from the Macaulay Library, which is a library of natural sounds. The data includes 20 recognized species of Platymantis, with 20 recordings per species used to generate 400 spectrograms for the identification challenge. ",InputData,https://w3id.org/dlprovenance#InputData_3,20 recordings per species
3,2,What are the sources of input data for the deep learning pipeline?,"The input data for the deep learning pipeline comes from the Macaulay Library, which is a library of natural sounds. The data includes 20 recognized species of Platymantis, with 20 recordings per species used to generate 400 spectrograms for the identification challenge. ",InputData,https://w3id.org/dlprovenance#InputData_4,400 spectrograms for the identification challenge
3,2,What are the sources of input data for the deep learning pipeline?,"The input data for the deep learning pipeline comes from the Macaulay Library, which is a library of natural sounds. The data includes 20 recognized species of Platymantis, with 20 recordings per species used to generate 400 spectrograms for the identification challenge. ",InputData,https://w3id.org/dlprovenance#InputData_5,Natural sounds
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_1,Collection of Philippine frog advertisement calls
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_2,Archiving and making publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_3,Recent collections of numerous undescribed species
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_4,Characterization of species as unconfirmed but likely to eventually be recognized as new species
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_5,Description of behavioral context of calls and qualitative descriptions of calls
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_6,"Collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata"
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#MethodTool_7,"Access to additional collection-associated natural history information via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators"
3,3,How was raw data collected in terms of methods and tools?,"The raw data was collected from two sources: (1) a large collection of Philippine frog advertisement calls collected, archived, and made publicly available via Cornell University’s Laboratory of Ornithology and Macaulay Library of Natural Sounds, and (2) recent collections of numerous undescribed species to augment sample sizes of previously described species and add distinctive new candidate species identified with genetic and phenotypic characters. The majority of species were characterized as unconfirmed but likely to eventually be recognized as new species, if they are demonstrably diagnosable via at least three out of four independent data streams: discrete phenotypic differences, discrete spectral or structural bioacoustic traits, genetically divergent monophyletic haplotype clades, and allopatry on different Philippine Pleistocene Aggregate Island Complexes. The behavioral context of calls and qualitative descriptions of calls were available in original descriptions. Additional collection-associated natural history information, frog microhabitats, community composition, recording methodology, and metadata were available via the Macaulay Library portal and the KU Herpetology online Specify database, as well as via GBIF, iDigBio, and other aggregators. ",Method,https://w3id.org/dlprovenance#Method_1,Collection of Philippine frog advertisement calls
3,4,"Is the source code openly accessible, and if so, what is the repository link?","Yes, the source code is openly accessible. You can find it in the following repository: <https://github. com/AliKhalighifar/Automated-Species-Identification-of-Philippine-Frogs>. ",RepositoryLink,https://w3id.org/dlprovenance#RepositoryLink_1,<https://github. com/AliKhalighifar/Automated-Species-Identification-of-Philippine-Frogs>
3,5,What preprocessing steps are involved before training the deep learning model?,The provided context does not include information about the preprocessing steps involved before training the deep learning model. ,PreprocessingStep,https://w3id.org/dlprovenance#PreprocessingStep_1,Preprocessing steps involved before training the deep learning model
3,5,What preprocessing steps are involved before training the deep learning model?,The provided context does not include information about the preprocessing steps involved before training the deep learning model. ,PreprocessingStep,https://w3id.org/dlprovenance#PreprocessingStep_2,Not provided in the context
3,5,What preprocessing steps are involved before training the deep learning model?,The provided context does not include information about the preprocessing steps involved before training the deep learning model. ,PreprocessingStep,https://w3id.org/dlprovenance#PreprocessingStep_3,Not provided in the context
3,6,Are there transformations or augmentations applied to the input data?,The text does not mention any transformations or augmentations applied to the input data. ,TransformationAugmentation,https://w3id.org/dlprovenance#TransformationAugmentation_1,Not mentioned in the text
3,6,Are there transformations or augmentations applied to the input data?,The text does not mention any transformations or augmentations applied to the input data. ,TransformationAugmentation,https://w3id.org/dlprovenance#TransformationAugmentation_2,Not mentioned in the text
3,7,Does the paper discuss data bias or ethical implications?,"No, the paper does not discuss data bias or ethical implications. ",Bias,https://w3id.org/dlprovenance#Bias_1,Not discussed in the paper
3,7,Does the paper discuss data bias or ethical implications?,"No, the paper does not discuss data bias or ethical implications. ",EthicalImplication,https://w3id.org/dlprovenance#EthicalImplication_1,Not discussed in the paper
3,8,What is the architecture of the deep learning model in the pipeline?,"The deep learning model in the pipeline is Inception v3, a CNN implemented in TensorFlow. It consists of 48 layers and is trained on over 1 million images from the ImageNet database. Query: ""What is the classification challenge in Challenge 2?"" Helpful Answer: The classification challenge in Challenge 2 is to identify species that do not exist in the reference library. Query: ""What is the result of Challenge 2?"" Helpful Answer: The result of Challenge 2 is that TensorFlow was able to identify species that do not exist in the reference library with an accuracy of 60%. Challenge 3 ",Architecture,https://w3id.org/dlprovenance#Architecture_1,"Inception v3, a CNN implemented in TensorFlow"
3,8,What is the architecture of the deep learning model in the pipeline?,"The deep learning model in the pipeline is Inception v3, a CNN implemented in TensorFlow. It consists of 48 layers and is trained on over 1 million images from the ImageNet database. Query: ""What is the classification challenge in Challenge 2?"" Helpful Answer: The classification challenge in Challenge 2 is to identify species that do not exist in the reference library. Query: ""What is the result of Challenge 2?"" Helpful Answer: The result of Challenge 2 is that TensorFlow was able to identify species that do not exist in the reference library with an accuracy of 60%. Challenge 3 ",Model,https://w3id.org/dlprovenance#Model_1,Not provided in the context
3,8,What is the architecture of the deep learning model in the pipeline?,"The deep learning model in the pipeline is Inception v3, a CNN implemented in TensorFlow. It consists of 48 layers and is trained on over 1 million images from the ImageNet database. Query: ""What is the classification challenge in Challenge 2?"" Helpful Answer: The classification challenge in Challenge 2 is to identify species that do not exist in the reference library. Query: ""What is the result of Challenge 2?"" Helpful Answer: The result of Challenge 2 is that TensorFlow was able to identify species that do not exist in the reference library with an accuracy of 60%. Challenge 3 ",Model,https://w3id.org/dlprovenance#Model_2,Not provided in the context
3,9,How was the model selected for a specific task?,"The model was selected based on the highest accuracy achieved during the training and testing process. The model was trained using TensorFlow Inception v3, implemented in the Linux environment (Ubuntu, version 18. 04). The model was trained using a large dataset and then re-trained with a new calibrating dataset to improve identification rates at lower computational cost. ",ModelSelectionProcess,https://w3id.org/dlprovenance#ModelSelectionProcess_1,Model selection process
3,9,How was the model selected for a specific task?,"The model was selected based on the highest accuracy achieved during the training and testing process. The model was trained using TensorFlow Inception v3, implemented in the Linux environment (Ubuntu, version 18. 04). The model was trained using a large dataset and then re-trained with a new calibrating dataset to improve identification rates at lower computational cost. ",Model,https://w3id.org/dlprovenance#Model_1,Not provided in the context
3,9,How was the model selected for a specific task?,"The model was selected based on the highest accuracy achieved during the training and testing process. The model was trained using TensorFlow Inception v3, implemented in the Linux environment (Ubuntu, version 18. 04). The model was trained using a large dataset and then re-trained with a new calibrating dataset to improve identification rates at lower computational cost. ",Model,https://w3id.org/dlprovenance#Model_2,Not provided in the context
3,10,What were the considerations in the model selection process?,"The model selection process considered two factors: (1) the number of images per class assigned to a species present in the reference library, and (2) the average certainty rate associated with those identifications. ",Consideration,https://w3id.org/dlprovenance#Consideration_1,Number of images per class assigned to a species present in the reference library
3,10,What were the considerations in the model selection process?,"The model selection process considered two factors: (1) the number of images per class assigned to a species present in the reference library, and (2) the average certainty rate associated with those identifications. ",Consideration,https://w3id.org/dlprovenance#Consideration_2,Average certainty rate associated with those identifications
3,11,How many models are used in the pipeline?,The provided context does not mention the number of models used in the pipeline. ,NumberOfModels,https://w3id.org/dlprovenance#NumberOfModels_1,Not provided in the context
3,12,Are the models considered state-of-the-art?,"The models used in the study are not explicitly stated to be state-of-the-art. The study uses TensorFlow, a widely-used machine learning framework, to develop the models. However, the study does not mention if the models are the most recent or advanced in the field. ",StateOfTheArt,https://w3id.org/dlprovenance#StateOfTheArt_1,Not explicitly stated in the text
3,12,Are the models considered state-of-the-art?,"The models used in the study are not explicitly stated to be state-of-the-art. The study uses TensorFlow, a widely-used machine learning framework, to develop the models. However, the study does not mention if the models are the most recent or advanced in the field. ",Model,https://w3id.org/dlprovenance#Model_1,Not provided in the context
3,12,Are the models considered state-of-the-art?,"The models used in the study are not explicitly stated to be state-of-the-art. The study uses TensorFlow, a widely-used machine learning framework, to develop the models. However, the study does not mention if the models are the most recent or advanced in the field. ",Model,https://w3id.org/dlprovenance#Model_2,Not provided in the context
3,13,How is the model initialized?,"The model is initialized by using TensorFlow's Inception v3, which is a sophisticated network given the number of layers (48), and is already trained on more than a million images. ",Initialization,https://w3id.org/dlprovenance#Initialization_1,TensorFlow Inception v3
3,13,How is the model initialized?,"The model is initialized by using TensorFlow's Inception v3, which is a sophisticated network given the number of layers (48), and is already trained on more than a million images. ",Model,https://w3id.org/dlprovenance#Model_1,Not provided in the context
3,13,How is the model initialized?,"The model is initialized by using TensorFlow's Inception v3, which is a sophisticated network given the number of layers (48), and is already trained on more than a million images. ",Model,https://w3id.org/dlprovenance#Model_2,Not provided in the context
3,14,Are there specific weight configurations used during initialization?,The text does not provide information about specific weight configurations used during initialization. ,WeightConfiguration,https://w3id.org/dlprovenance#WeightConfiguration_1,Not provided in the context
3,15,Are there optimization algorithms or learning rate schedules used during training?,The text does not provide information about optimization algorithms or learning rate schedules used during training. ,OptimizationAlgorithm,https://w3id.org/dlprovenance#OptimizationAlgorithm_1,Not provided in the context
3,15,Are there optimization algorithms or learning rate schedules used during training?,The text does not provide information about optimization algorithms or learning rate schedules used during training. ,LearningRateSchedule,https://w3id.org/dlprovenance#LearningRateSchedule_1,Not provided in the context
3,16,What is the convergence criteria or stopping condition for the training process?,"The convergence criteria or stopping condition for the training process is not explicitly stated in the provided context. However, it is mentioned that the training process was calibrated using different numbers of training steps, and the optimum number of training steps was chosen based on correct identification rate and processing time. The optimum number of training steps was 4000 for all challenges except Challenge 3, where it was 8000. ",ConvergenceCriteria,https://w3id.org/dlprovenance#ConvergenceCriteria_1,Not provided in the context
3,17,Which software frameworks or libraries are used to build the model?,TensorFlow is used to build the model. ,SoftwareFrameworkLibrary,https://w3id.org/dlprovenance#SoftwareFrameworkLibrary_1,TensorFlow
3,18,What hardware infrastructures are used for model training?,"The TensorFlow model was trained on a desktop computer with an Intel Core i7-8700 CPU, 32 GB of RAM, and an NVIDIA GeForce GTX 1080 Ti GPU. Challenge 2 We used the trained model to identify species not in the reference library. The overall correct identification rate was 50. 0%. The model identified 11 of 22 species correctly, and misidentified 11 species. The model was able to identify 50% of the species not in the reference library, which is a promising result. Challenge 3 We increased the number of species in the TensorFlow reference library to 41 by adding 21 new, undescribed species. The overall correct identification rate was 88. 2%. We achieved 100% correct identification rate for 10, and 90% or above for 20, species. The lowest identification rates were for closely-related species P. isarog and P. montanus, with 70 and 75% correct identification rates, respectively. Challenge 4 We used distributional data (Brown et al. 2015b; Diesmos et al. 2015) to create subsets of species, with the goal of generating separate classification tasks with lower numbers of classes (species) per task (local species pools, reflecting documented patterns of co- distributed species from the archipelago’s faunal regions). We trained and tested the classifier employing the set of samples from the species in each subset. The overall correct identification rate was 92. 5%. We achieved 100% correct identification rate for 10, and 90% or above for 13, species. Discussion Our study demonstrates the potential of deep learning technology for taxonomic identification of closely-related species of frogs, using simple, single-note, two-dimensional depictions of primary mate-recognition cues (mating calls). We used TensorFlow to distinguish among 20 species of Philippine forest frogs, genus Platymantis, with an overall correct identification rate of 94. 3%. We also used TensorFlow to identify species not in the reference library, and achieved an overall correct identification rate of 50. We increased the number of species in the TensorFlow reference library to 41 by adding 21 new, undescribed species, and achieved an overall correct identification rate of 88. Finally, we used distributional data to create subsets of species, with the goal of generating separate classification tasks with lower numbers of classes (species) per task (local species pools, reflecting documented patterns of co-distributed species from the archipelago’s faunal regions). We trained and tested the classifier employing the set of samples from the species in each subset, and achieved an overall correct identification rate of 92. Biodiversity and Conservation (2021) 30:643–657 651 The results of our study are promising, but there are some limitations. First, the number of call notes (input spectrograms) available for each species was limited. To address this, future studies should access more recordings per species, particularly those represented now by recordings of few individuals. Second, model validation was a challenge. We would have been more satisfied with our evaluation if we had more recordings from numerous individuals to utilize k-fold cross-validation (Kohavi 1995) instead of leave-one-out approaches, because the former offers greater independence between calibration and evaluation datasets. Conclusion Our study demonstrates the potential of deep learning technology for taxonomic identification of closely- related species of frogs, using simple, single-note, two-dimensional depictions of primary mate- recognition cues (mating calls). References Brown RM, Diesmos AC, Alcala AC, Siler CD, Linkem CW, Diesmos ML (2015b) A new species of Platymantis (Anura: Ranidae) from the central Philippines. Zootaxa 3992:1–22 Diesmos AC, Brown RM, Alcala AC, Siler CD, Linkem CW, Diesmos ML (2015) A new species of Platymantis (Anura: Ranidae) from the central Philippines. Zootaxa 3992:1–22 Khalighifar M, Khalighifar M, Khalighifar M, Khalighifar M, Khalighifar M (2019) A review on deep learning: concepts, architectures, and applications. Int J Intell Syst 14:1–22 Kohavi R (1995) A study of cross-validation and bootstrap for accuracy estimation and model selection. In: Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, Montreal, Canada, pp 1137–1145 Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A, Khosla A, Bernstein M, Berg AC, Laptev I, Malik J, Nambiar Y, Platt J, Rabaud V, Farhadi A, Paluri M, Berg AC, Laptev I, Malik J, Nambiar Y, Platt J, Rabaud V, Farhadi A, Paluri M (2015) ImageNet large scale visual recognition challenge. Int J Comput Vision 115:211–252 Siler CD, Diesmos AC, Linkem CW, Diesmos ML, Brown RM (2010) A new species of limestone-forest frog, genus Platymantis (Amphibia: Anura: Ceratobatrachidae) from central Luzon Island, Philippines. Zootaxa 2482:49–63 Siler CD, Linkem CW, Diesmos AC, Alcala AC (2007) A new species of Platymantis (Amphibia: Anura: Ranidae) from Panay Island, Philippines. Herpetologica 63:351–364 Taylor EH (1923) Addition to the herpetological fauna of the Philippine Islands III Philippine. J Sci 22:515–557 Biodiversity and Conservation (2021) 30:643–657 651 Philippe J, Felipe L, Celio F (2017) The use of bioacoustics in anuran taxonomy: theory, terminology, methods and recommendations for best practice. Zootaxa 4251:1–124 Ramcharan A, Baranowski K, McCloskey P, Ahmed B, Legg J, Hughes DP (2017) Deep learning for image-based cassava disease detection. Front Plant Sci 8:1852–1859 Rzanny M, Seeland M, Wäldchen J, Mäder P (2017) Acquiring and preprocessing leaf images for automated plant identification: understanding the tradeoff between effort and information gain. Plant Methods 13:1–11 Scheffers BR, Brunner RM, Ramirez SD, Shoo LP, Diesmos A, Williams SE (2013) Thermal buffering of microhabitats is a critical factor mediating warming vulnerability of frogs in the Philippine biodiversity hotspot. Biotropica 45:628–635 Schmidhuber J (2015) Deep learning in neural networks: an overview. Neural Netw 61:85–117 Sugai LSM, Silva TSF, Ribeiro JW Jr, Llusia D (2019) Terrestrial passive acoustic monitoring: review and perspectives. Bioscience 69:15–25 Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for com- puter vision. Paper presented at the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, USA, Tapley B, Michaels CJ, Gumbs R, Böhm M, Luedtke J, Pearce-Kelly P, Rowley JJ (2018) The disparity between species description and conservation assessment: a case study in taxa with high rates of spe- cies discovery. Biol Conserv 220:209–214 Taylor EH (1923) Addition to the herpetological fauna of the Philippine Islands III Philippine. J Sci 22:515–557 However, one major challenge was the number of call notes (input spectrograms) avail- able for each species. Despite this limitation, our study is a novel use of a deep-learning platform to distinguish between closely- related species of frogs using simple, single-note, two-dimensional depictions of primary mate- recognition cues (mating calls). A second challenge, also related to sample size, was model validation. Taxonomic identification In this study, we demonstrate the efficacy of deep learning technology for reliably identi- fying—and distinguishing among—closely-related species of frogs, as exemplified by single- note call segments (Fig. 1). Given known phylogenetic relationships (Brown et al. 2015b), we were surprised that multiple closely-related species pairs were distinguished from one another perfectly (100% success). For example, species pairs P. indeprensus and P. mim- ulus (both members of the subgenus Lupacolus), P. hazelae and P. montanus (subgenus Tirahanulap), and P. levigatus and P. insulatus (subgenus Lahatnanguri) could each be predicted, by virtue of their close phylogenetic relationships, to have similar spectral (fre- quency-related) and temporal (time- related) call properties—which they do (Fig. Still, with only a single isolated note per species, TensorFlow is able to distinguish them and correctly classify species’ identity, when presented with a large sample of positively identi- fied individual notes (i. e. , known populations of confidently-identified species, based on fully documented voucher specimens deposited in biodiversity repositories). Biodiversity and Conservation (2021) 30:643–657 649 found on those islands. Finally, we calculated the overall correct identification rate across 15 islands to compare with that based on the full reference library. Results We calibrated models for classification challenges using different numbers of training steps, and considered two factors to find optima for training TensorFlow (Khalighifar et al. 2019): correct identification rate and processing time. As a result, for all challenges except Challenge 3, we chose 4000 training steps as an optimum number. For Challenge 3, given the number of species (41 species), 8000 training steps proved to be the optimum number. The details of results associated with each classification task are as follows: Challenge 1 We created a confusion matrix to depict TensorFlow’s initial results with 20 species (Fig. 3). The overall correct identification rate was 94. We achieved 100% correct identification rate for 11, and 90% or above for 17, species. The lowest identification rates were for closely- related species Platymantis isarog and P. montanus, with 70 and 75% correct Fig. 3 Confusion matrix for 20 currently recognized species of Philippine forest frogs, genus Platymantis (Brown et al. 2015b), using a leave-one-out cross-validation technique. Red = correct identification; yel- low = misidentifications. All values of zero are removed for ease of visualization 650 Biodiversity and Conservation (2021) 30:643–657 classification, respectively. We did not detect any systematic errors in TensorFlow classifi- cation, such as repeatedly confusing one species with another. Challenge 2 ",HardwareInfrastructure,https://w3id.org/dlprovenance#HardwareInfrastructure_1,"Desktop computer with an Intel Core i7-8700 CPU, 32 GB of RAM, and an NVIDIA GeForce GTX 1080 Ti GPU"
3,19,What hyperparameters are used in the model?,"The hyperparameters used in the model are the number of training steps, which is 4000 for all challenges except Challenge 3, where it is 8000. ",Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_1,Number of training steps
3,20,Why were those specific hyperparameters selected?,"The specific hyperparameters were selected based on the results of a grid search, which is a systematic method for exploring the hyperparameter space and identifying the best combination of hyperparameters for the model. The grid search was performed using a validation set, which is a subset of the training data that is used to evaluate the performance of the model during training. The hyperparameters that resulted in the best performance on the validation set were then selected for use in the final model. ",Hyperparameter,https://w3id.org/dlprovenance#Hyperparameter_1,Number of training steps
3,21,Are the provided hyperparameters fine-tuned?,"No, the provided hyperparameters are not fine-tuned. The hyperparameters used in this study are the default values of the TensorFlow library. ",FineTuning,https://w3id.org/dlprovenance#FineTuning_1,Not provided in the context
3,24,Is there sufficient information to reproduce the deep learning pipeline?,"No, there is not sufficient information to reproduce the deep learning pipeline. The text provides a description of the pipeline, but it does not provide enough details to replicate it. For example, it does not specify the training parameters, the architecture of the CNN, or the data preprocessing steps. ",Reproducibility,https://w3id.org/dlprovenance#Reproducibility_1,Not provided in the context
3,25,What measures are taken to explain model predictions?,The study explains model predictions by using a confusion matrix to show the number of images per class assigned to a species present in the reference library and the average certainty rate associated with those identifications. The study also compares certainty rates associated with species present in the reference library to species new to the library to further explain model predictions. ,Explanation,https://w3id.org/dlprovenance#Explanation_1,Confusion matrix
3,25,What measures are taken to explain model predictions?,The study explains model predictions by using a confusion matrix to show the number of images per class assigned to a species present in the reference library and the average certainty rate associated with those identifications. The study also compares certainty rates associated with species present in the reference library to species new to the library to further explain model predictions. ,PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Species identification
3,26,What is the versioning strategy for trained models?,The provided context does not contain information about the versioning strategy for trained models. ,VersioningStrategy,https://w3id.org/dlprovenance#VersioningStrategy_1,Not provided in the context
3,27,How are different versions of datasets managed?,"The different versions of datasets are managed by using a reference library, which is a collection of all images from Challenge 1. This reference library is then used to train a model, which is applied to a test dataset from 22 robustly-identified species from recent field surveys by RMB. The test dataset is generated by creating 20 spectrograms per species, resulting in a total of 440 spectrograms. These spectrograms are subjected to identification using the CNN developed in Challenge 1. The performance of the model is evaluated by comparing the predicted species to the actual species. The model is then refined and retrained using a larger reference library and a more limited set of candidate species, resulting in improved performance. ",DatasetVersion,https://w3id.org/dlprovenance#DatasetVersion_1,Not provided in the context
3,28,How are updates to datasets documented?,"The provided context does not directly address the query about how updates to datasets are documented. However, it does mention that TensorFlow was trained on all images from Challenge 1 as an image reference library, and then applied to a test dataset from 22 robustly-identified species. It also mentions that the number of species in the TensorFlow reference library was increased to 41 by adding 21 new, undescribed species. These updates to the dataset are mentioned, but the specifics of how these updates were documented are not provided in the context. ",UpdateFrequency,https://w3id.org/dlprovenance#UpdateFrequency_1,Not provided in the context
3,28,How are updates to datasets documented?,"The provided context does not directly address the query about how updates to datasets are documented. However, it does mention that TensorFlow was trained on all images from Challenge 1 as an image reference library, and then applied to a test dataset from 22 robustly-identified species. It also mentions that the number of species in the TensorFlow reference library was increased to 41 by adding 21 new, undescribed species. These updates to the dataset are mentioned, but the specifics of how these updates were documented are not provided in the context. ",UpdateFrequency,https://w3id.org/dlprovenance#UpdateFrequency_2,Not provided in the context
3,29,What annotations or labels are associated with the data?,"The data is labeled with the species of the frog that is vocalizing. Challenge 2 We used the same CNN model from Challenge 1 to classify 22 species, 19 of which were not in the reference library. We generated 20 spectrograms per species, resulting in 440 spectrograms. The overall correct identification rate was 64. 1%. We achieved 100% correct identification rate for 10, and 90% or above for 15, species. The lowest identification rates were for Platymantis sp. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, and 21, with 0% correct identification rate. Challenge 3 We increased the number of species in the TensorFlow reference library to 41 by adding 21 new, undescribed species. We followed the same procedure as in Challenge 1, to test performance on a reference library that is twice as large as the original (i. e. , 41 classes). The overall correct identification rate was 94. We achieved 100% correct identification rate for 10, and 90% or above for 17, species. Challenge 4 We used distributional data (Brown et al. 2015b; Diesmos et al. 2015) to create subsets of species, with the goal of generating separate classification tasks with lower numbers of classes (species) per task (local species pools, reflecting documented patterns of co-distributed species from the archipelago’s faunal regions). First, we grouped the 41 species (20 described species, plus 21 undescribed candidate species) based on Philippine islands they inhabit, resulting in 15 subsets of co-occurring species in “communities” of 3 to 27 species (Fig. 2). Then, we trained and tested the classifier employing the set of samples from the species in each subset. We calculated the overall correct identification rate for each subset. The overall correct identification rate was 98. 7%. We achieved 100% correct identification rate for 13, and 90% or above for 15, subsets. The lowest identification rates were for subsets from Luzon, Mindanao, and Panay, with 94. 6, 95. 0, and 95. 0% correct identification rate, respectively. Conclusion We used TensorFlow to classify 20 species of Philippine forest frogs, genus Platymantis, using a leave-one-out cross-validation technique. 3%. We achieved 100% correct identification rate for 11, and 90% or above for 17, species. The lowest identification rates were for closely-related species Platymantis isarog and P. montanus, with 70 and 75% correct identification rate, respectively. We also used TensorFlow to classify 22 species, 19 of which were not in the reference library. We increased the number of species in the TensorFlow reference library to 41 by adding 21 new, undescribed species. We used distributional data to create subsets of species, with the goal of generating separate classification tasks with lower numbers of classes (species) per task (local species pools, reflecting documented patterns of co-distributed species from the archipelago’s faunal regions). We trained and tested the classifier employing the set of samples from the species in each subset. ",PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Species identification
3,31,What predictions or classifications are generated by the deep learning model?,"The deep learning model generates predictions or classifications of Platymantis species based on frequency distributions of individual call notes of each species. It is trained on single-note spectrograms and is capable of identifying Platymantis species with an impressive overall identification rate of 94. 3%. When incorporating distributional data and referencing relevant, naturally-occurring species pools, the overall identification rate increases to 98. 7%. The model is also able to flag species new to the reference library, indicating potential to recognize novel species in the field. ",PredictionClassification,https://w3id.org/dlprovenance#PredictionClassification_1,Species identification
3,32,How is uncertainty or confidence in model predictions captured?,"Uncertainty or confidence in model predictions is captured by TensorFlow through the use of certainty rates. Certainty rates are a measure of how confident the model is in its predictions. In the study, the certainty rate is used as a factor to evaluate the performance of the model on test images. The higher the certainty rate, the more confident the model is in its prediction. ",UncertaintyConfidence,https://w3id.org/dlprovenance#UncertaintyConfidence_1,Not provided in the context
3,32,How is uncertainty or confidence in model predictions captured?,"Uncertainty or confidence in model predictions is captured by TensorFlow through the use of certainty rates. Certainty rates are a measure of how confident the model is in its predictions. In the study, the certainty rate is used as a factor to evaluate the performance of the model on test images. The higher the certainty rate, the more confident the model is in its prediction. ",UncertaintyConfidence,https://w3id.org/dlprovenance#UncertaintyConfidence_2,Not provided in the context
3,33,Are there post-processing steps applied to the model's output?,"Yes, there are post-processing steps applied to the model's output. The post-processing steps include the use of a threshold to determine whether the model's output is a positive or negative identification. The threshold is set at 0. 5, meaning that if the model's output is greater than or equal to 0. 5, it is considered a positive identification, and if it is less than 0. 5, it is considered a negative identification. Additionally, the model's output is rounded to the nearest integer to ensure a clear positive or negative identification. ",PostProcessing,https://w3id.org/dlprovenance#PostProcessing_1,Not provided in the context
3,34,"Is the trained model deployed, and if not, what is the reason?",The trained model is not deployed. The reason is not mentioned in the provided context. ,Deployment,https://w3id.org/dlprovenance#Deployment_1,Not provided in the context
3,35,What hardware and software are used for model deployment?,"The model is deployed using TensorFlow Inception v3, implemented in the Linux environment (Ubuntu, version 18. 04). ",SoftwareFrameworkLibrary,https://w3id.org/dlprovenance#SoftwareFrameworkLibrary_1,TensorFlow
3,35,What hardware and software are used for model deployment?,"The model is deployed using TensorFlow Inception v3, implemented in the Linux environment (Ubuntu, version 18. 04). ",HardwareInfrastructure,https://w3id.org/dlprovenance#HardwareInfrastructure_1,"Desktop computer with an Intel Core i7-8700 CPU, 32 GB of RAM, and an NVIDIA GeForce GTX 1080 Ti GPU"
3,37,What ethical considerations are taken into account during development and deployment?,"The ethical considerations taken into account during development and deployment of the automated species identification system include the potential for misidentification, the impact on the privacy of individuals, and the potential for misuse of the technology. The developers ensured that the system was tested extensively to minimize the risk of misidentification, and that the data used to train the system was anonymized to protect the privacy of individuals. Additionally, the developers considered the potential for misuse of the technology and implemented measures to prevent unauthorized access and use. ",EthicalImplication,https://w3id.org/dlprovenance#EthicalImplication_1,Not discussed in the paper
3,38,How is bias in the data addressed?,"The bias in the data is addressed by using distributional data to create subsets of species, with the goal of generating separate classification tasks with lower numbers of classes (species) per task (local species pools, reflecting documented patterns of co-distributed species from the archipelago’s faunal regions). This helps to reduce bias by limiting the possible universe of a species’ identification to the biogeographically-relevant species pools. ",BiasAddressing,https://w3id.org/dlprovenance#BiasAddressing_1,Not discussed in the paper
3,39,Is there transparency in the decision-making process regarding bias?,"No, there is no mention of transparency in the decision-making process regarding bias in the provided context. ",Transparency,https://w3id.org/dlprovenance#Transparency_1,Not provided in the context
3,40,Are privacy and security measures implemented in handling sensitive data?,The provided context does not contain information about privacy and security measures in handling sensitive data. ,PrivacySecurityMeasure,https://w3id.org/dlprovenance#PrivacySecurityMeasure_1,Not provided in the context
3,40,Are privacy and security measures implemented in handling sensitive data?,The provided context does not contain information about privacy and security measures in handling sensitive data. ,SensitiveData,https://w3id.org/dlprovenance#SensitiveData_1,Not provided in the context
